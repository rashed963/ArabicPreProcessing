import regex as re
import nltk
import os
from ArabicPreProcessing.helper import Helper

class GeneralProcessingService:
    emojies_uni = {

        u': üòÇ :': u'üòÇ',
        u': ü§£ :': u'ü§£',
        u': üòÜ :': u'üòÜ',
        u': üòÖ :': u'üòÖ',
        u': üòπ :': u'üòπ',
        u': üòõ :': u'üòõ',
        u': üòú :': u'üòú',
        u': üòù :': u'üòù',
        u': üê∏ :': u'üê∏',
        u': üêç :': u'üêç',

        u': üòÄ :': u'üòÄ',
        u': üòÅ :': u'üòÅ',
        u': üòÉ :': u'üòÉ',
        u': üòÑ :': u'üòÑ',
        u': üò∏ :': u'üò∏',
        u': üò∫ :': u'üò∫',
        u': ü§≠ :': u'ü§≠',
        u': üòá :': u'üòá',
        u': ü§† :': u'ü§†',
        u': üòä :': u'üòä',
        u': ü§ó :': u'ü§ó',
        u': üòé :': u'üòé',
        u': üåö :': u'üåö',
        u': üåù :': u'üåù',
        u': üòâ :': u'üòâ',
        u': üòã :': u'üòã',
        u': ü§§ :': u'ü§§',
        u': ü§© :': u'ü§©',
        u': ü•≥ :': u'ü•≥',
        u': üî• :': u'üî•',
        u': üòå :': u'üòå',
        u': ü§ë :': u'ü§ë',
        u': üíÉ :': u'üíÉ',
        u': üí™ :': u'üí™',
        u': ‚úî :': u'‚úî',
        u': ‚úîÔ∏è :': u'‚úîÔ∏è',
        u': ü§ì :': u'ü§ì',
        u': üßê :': u'üßê',
        u': üëç :': u'üëç',
        u': üëå :': u'üëå',
        u': üôÜ‚Äç‚ôÇÔ∏è :': u'üôÜ‚Äç‚ôÇÔ∏è',
        u': üôÜ‚Äç‚ôÄÔ∏è :': u'üôÜ‚Äç‚ôÄÔ∏è',
        u': üôã‚Äç‚ôÇÔ∏è :': u'üôã‚Äç‚ôÇÔ∏è',
        u': üôã‚Äç‚ôÄÔ∏è :': u'üôã‚Äç‚ôÄÔ∏è',
        u': ‚úåÔ∏è :': u'‚úåÔ∏è',
        u': ü§ô :': u'ü§ô',
        u': üëè :': u'üëè',
        u': üôå :': u'üôå',
        u': ü§ù :': u'ü§ù',
        u': üôà :': u'üôà',
        u': üôâ :': u'üôâ',
        u': üôä :': u'üôä',
        u': ‚ò∫Ô∏è :': u'‚ò∫Ô∏è',

        u': üòò :': u'üòò',
        u': üòó :': u'üòó',
        u': üòô :': u'üòô',
        u': üòö :': u'üòö',
        u': üòΩ :': u'üòΩ',
        u': üíã :': u'üíã',
        u': üëÑ :': u'üëÑ',
        u': ‚ù§Ô∏è :': u'‚ù§Ô∏è',
        u': ‚ù§Ô∏è :': u'‚ù§Ô∏è',
        u': üíò :': u'üíò',
        u': üíù :': u'üíù',
        u': üíñ :': u'üíñ',
        u': üíó :': u'üíó',
        u': üíì :': u'üíì',
        u': üíû :': u'üíû',
        u': üíï :': u'üíï',
        u': üíå :': u'üíå',
        u': ‚ù£Ô∏è :': u'‚ù£Ô∏è',
        u': üß° :': u'üß°',
        u': üíõ :': u'üíõ',
        u': üíö :': u'üíö',
        u': üíô :': u'üíô',
        u': üíú :': u'üíú',
        u': üñ§ :': u'üñ§',
        u': ‚ô• :': u'‚ô•',
        u': üíü :': u'üíü',
        u': üòç :': u'üòç',
        u': ü•∞ :': u'ü•∞',
        u': üòª :': u'üòª',
        u': üë®‚Äç‚ù§Ô∏è‚Äçüíã‚Äçüë® :': u'üë®‚Äç‚ù§Ô∏è‚Äçüíã‚Äçüë®',
        u': üë©‚Äç‚ù§Ô∏è‚Äçüíã‚Äçüë© :': u'üë©‚Äç‚ù§Ô∏è‚Äçüíã‚Äçüë©',
        u': üë®‚Äç‚ù§Ô∏è‚Äçüë® :': u'üë®‚Äç‚ù§Ô∏è‚Äçüë®',
        u': üë©‚Äç‚ù§Ô∏è‚Äçüë© :': u'üë©‚Äç‚ù§Ô∏è‚Äçüë©',
        u': üíë :': u'üíë',
        u': üíç :': u'üíç',
        u': üë∞ :': u'üë∞',
        u': üå∏ :': u'üå∏',
        u': üíÆ :': u'üíÆ',
        u': üèµ :': u'üèµ',
        u': üåπ :': u'üåπ',
        u': ü•Ä :': u'ü•Ä',
        u': üå∫ :': u'üå∫',
        u': üåª :': u'üåª',
        u': üåº :': u'üåº',
        u': üå∑ :': u'üå∑',
        u': üíê :': u'üíê',
        u': ‚úå :': u'‚úå',
        u': üòî :': u'üòî',
        u': üòì :': u'üòì',
        u': üòû :': u'üòû',
        u': üòü :': u'üòü',
        u': ‚òπÔ∏è :': u'‚òπÔ∏è',
        u': üôÅ :': u'üôÅ',
        u': üò¢ :': u'üò¢',
        u': üò≠ :': u'üò≠',
        u': üò¶ :': u'üò¶',
        u': üòß :': u'üòß',
        u': üòø :': u'üòø',
        u': üò• :': u'üò•',
        u': üò´ :': u'üò´',
        u': üò∞ :': u'üò∞',
        u': üò® :': u'üò®',
        u': ü•∫ :': u'ü•∫',
        u': üò© :': u'üò©',
        u': üíî :': u'üíî',
        u': ü§¶‚Äç‚ôÇÔ∏è :': u'ü§¶‚Äç‚ôÇÔ∏è',
        u': ü§¶‚Äç‚ôÄÔ∏è :': u'ü§¶‚Äç‚ôÄÔ∏è',
        u': üò™ :': u'üò™',
        u': üò¥ :': u'üò¥',
        u': üò∑ :': u'üò∑',
        u': ü§í :': u'ü§í',
        u': ü§ï :': u'ü§ï',
        u': ü§¢ :': u'ü§¢',
        u': ü§Æ :': u'ü§Æ',
        u': ü§ß :': u'ü§ß',
        u': üò∂ :': u'üò∂',
        u': ü§ê :': u'ü§ê',
        u': üòè :': u'üòè',
        u': üòº :': u'üòº',
        u': üòà :': u'üòà',
        u': üòí :': u'üòí',
        u': ü§® :': u'ü§®',
        u': üòê :': u'üòê',
        u': üòë :': u'üòë',
        u': üò¨ :': u'üò¨',
        u': üôÇ :': u'üôÇ',
        u': üôÉ :': u'üôÉ',
        u': üòï :': u'üòï',
        u': ü§î :': u'ü§î',
        u': üôÑ :': u'üôÑ',
        u': üòñ :': u'üòñ',
        u': üòµ :': u'üòµ',
        u': ü•¥ :': u'ü•¥',
        u': ü•µ :': u'ü•µ',
        u': üò± :': u'üò±',
        u': üò≥ :': u'üò≥',
        u': üòÆ :': u'üòÆ',
        u': üòØ :': u'üòØ',
        u': üôÄ :': u'üôÄ',
        u': üò≤ :': u'üò≤',
        u': üò° :': u'üò°',
        u': üò† :': u'üò†',
        u': ü§¨ :': u'ü§¨',
        u': üò§ :': u'üò§',
        u': ü§Ø :': u'ü§Ø',
        u': üòæ :': u'üòæ',
        u': üëø :': u'üëø',
        u': üò£ :': u'üò£',
        u': ü§• :': u'ü§•',
        u': ü§´ :': u'ü§´',
        u': üí© :': u'üí©',
        u': üñï :': u'üñï',
        u': üëä :': u'üëä',
        u': üëé :': u'üëé',
        u': üôÖ‚Äç‚ôÇÔ∏è :': u'üôÖ‚Äç‚ôÇÔ∏è',
        u': üôÜ‚Äç‚ôÄ :': u'üôÜ‚Äç‚ôÄ',
        u': üôÖ‚Äç‚ôÄÔ∏è :': u'üôÖ‚Äç‚ôÄÔ∏è',  ######
        u': üíÜ‚Äç‚ôÇ :': u'üíÜ‚Äç‚ôÇ',
        u': üíÅ :': u'üíÅ',
        u': ü§™ :': u'ü§™',
        u': ‚ù§ :': u'‚ù§',
        u': ‚òª :': u'‚òª',
        u': üôã‚Äç‚ôÄ :': u'üôã‚Äç‚ôÄ',
        u': ‚òª :': u'‚òª',
        u': üí£ :': u'üí£',
        u': üèÉ :': u'üèÉ',
        u': ü§∑‚Äç‚ôÇ :': u'ü§∑‚Äç‚ôÇ',
        u': ü§¶‚Äç‚ôÄ :': u'ü§¶‚Äç‚ôÄ',
        u': ‚ùå :': u'‚ùå',
        u': ‚òπ :': u'‚òπ',
        u': üîù :': u'üîù',
        u': üìø :': u'üìø',
        u': üéµ :': u'üéµ',
        u': üé∂ :': u'üé∂',
        u': üéº :': u'üéº',
        u': ‚úä :': u'‚úä',
        u': ü§¶‚Äç‚ôÇ :': u'ü§¶‚Äç‚ôÇ',
        u': üíÜüèª‚Äç‚ôÄ :': u'üíÜüèª‚Äç‚ôÄ',

    }
    emojies_dict = {v: k for k, v in emojies_uni.items()}

    def __init__(self, input_data=[],data_path=''):
        '''input data as list through input_data, or as file through data_path,
        output pre-processed data as list'''
        import pandas as pd

        self.data = pd.DataFrame()
        if len(input_data) != 0:
            self.data['content'] = input_data

        settings_dir = os.path.dirname('__file__')
        PROJECT_ROOT = os.path.abspath(os.path.dirname(settings_dir))
        stop_word_file_path = PROJECT_ROOT + '\\resources\\stopwords.csv'

        stop_wrods = pd.read_csv(stop_word_file_path, error_bad_lines=False, names=['stopword'])
        self.stop_words = list(stop_wrods['stopword'])
        
        if data_path != '':
            self.data['content'] = pd.read_csv(data_path, error_bad_lines=False, names=['content'])

        self.cleanedData = []
        self.hashTags = []
        self.excludedWords = []
        self.new_tokenized_data  = []

        tokenized_data = [nltk.word_tokenize(item) for item in self.data.content]
        for i in range(len(tokenized_data)):
            temp = []
            for j in range(len(tokenized_data[i])):
                if len(tokenized_data[i][j]) > 2 and  not(self.sum_func(tokenized_data[i][j]) ):
                    temp.append(tokenized_data[i][j])
            self.new_tokenized_data.append(temp)

    def sum_func(self,text):
        match = re.search(r'(([.?#@+])\2{2,})', text)
        return match

    def de_emojize(self):
        self.cleanedData = Helper(GeneralProcessingService.emojies_dict).deNoise(self.cleanedData)
        return

    def remove_names(self, lst):
        flatten =[item for sublist in lst for item in sublist]
        self.cleanedData = Helper(flatten).deNoise(self.cleanedData)
        return

    def initial_pre_processing(self):
        temp = list((self.data['content']))
        self.cleanedData += [re.sub(r'[A-Za-z]+|[\u0400-\u0500]+', '', str(item)) for item in temp]
        return

    def remove_hashtags(self, first_time=1):
        if first_time == 1:
            for i in range(len(self.cleanedData)):
                self.cleanedData[i] = str(' '.join(
                    re.sub("([0-9]+)|([A-Za-z]+)|\_+|(\#)+|(\/)+|(\:)+", " ", self.cleanedData[i]).split()))
            return

    def remove_mentions(self, mentions=[]):
        WORD = re.compile(r'\w+')

        flatten = []
        for i in range(len(mentions)):
            temp = [c for c in WORD.findall(mentions[i])]
            if len(temp) > 1:
                for j in temp:
                    flatten.append(j)
            elif len(temp) == 1:
                flatten.append(temp[0])
        for i in range(len(self.cleanedData)):
            # self.hashTags.append(re.findall(r"#(\w+)", str(i)))
            # self.data['content'][i] = str(' '.join(
            #     c for c in WORD.findall(self.data['content'][i]) if c not in flatten and c != ''))
            self.cleanedData[i] = str(' '.join(
                c for c in WORD.findall(self.cleanedData[i]) if c not in flatten and c != ''))
        return

    def de_noise(self):
        from string import punctuation
        noise = re.compile(""" Ÿë    | # Tashdid
                                 Ÿé    | # Fatha
                                 Ÿã    | # Tanwin Fath
                                 Ÿè    | # Damma
                                 Ÿå    | # Tanwin Damm
                                 Ÿê    | # Kasra
                                 Ÿç    | # Tanwin Kasr
                                 Ÿí    | # Sukun
                                 ŸÄ   |  # Tatwil/Kashida
                                 ÿå   |
                             """, re.VERBOSE)
        flagsUs = re.compile("["
                             u"\U0001F680-\U0001F6FF"  # transport & map symbols
                             u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                             "]+", flags=re.UNICODE)
        dirtyChars = re.compile("["
                                "\u0600-\u0620"
                                "\u063B-\u0640"
                                "\u064B-\u065F"
                                "\u066A-\u06FF"
                                "\u0750-\u077F"
                                "\u08A0-\u08FF"
                                "\uFB50-\uFBE9"
                                "\uFBF0-\uFBFB"
                                "\uFC5B-\uFC63"
                                "\uFCF2-\uFCF4"
                                "\uFD3C-\uFD4F"
                                "\uFD90-\uFD91"
                                "\uFDC8-\uFDFF"
                                "\uFE70-\uFE7F"
                                "\uFEFD-\uFEFF"
                                "]+", flags=re.UNICODE)
        for i in range(len(self.cleanedData)):
            self.cleanedData[i] = str(re.sub(flagsUs, '', self.cleanedData[i]))
            self.cleanedData[i] = str(re.sub(dirtyChars, '', self.cleanedData[i]))
            self.cleanedData[i] = str(re.sub(noise, '', self.cleanedData[i]))
            self.cleanedData[i] = str(''.join(c for c in self.cleanedData[i] if c not in punctuation))
        return

    def remove_stop_words(self):
        for i in range(len(self.cleanedData)):
            self.cleanedData[i] = (
                str(' '.join(
                    c for c in nltk.word_tokenize(self.cleanedData[i]) if c not in self.stop_words and c != '')))
        return

    def normalize(self):
        for i in range(len(self.cleanedData)):
            self.cleanedData[i] = re.sub("[ÿ•ÿ£Ÿ±ÿ¢ÿß]", "ÿß", self.cleanedData[i])
            self.cleanedData[i] = re.sub("Ÿâ", "Ÿä", self.cleanedData[i])
            self.cleanedData[i] = re.sub("ÿ§", "ÿ°", self.cleanedData[i])
            self.cleanedData[i] = re.sub("[Ôª∑]", "ŸÑÿß", self.cleanedData[i])
            self.cleanedData[i] = re.sub("ÿ©", "Ÿá", self.cleanedData[i])
            self.cleanedData[i] = re.sub("ÿ¶", "ÿ°", self.cleanedData[i])
            self.cleanedData[i] = ''.join([i for i in self.cleanedData[i] if not i.isdigit()])
            self.cleanedData[i] = re.sub(r'([\u0600-\u06FF])\1{3,}', r'\1\1\1', self.cleanedData[i])
            self.cleanedData[i] = re.sub(
                r'([üòÇü§™ü§£üòÜüòÖüòπüòõüòúüòùüê∏üêçüòÄüòÅüòÉüòÑüò∏üò∫ü§≠üòáü§†üòäü§óüòéüåöüåùüòâüòãü§§ü§©ü•≥üî•üòåü§ëüíÉüí™‚úî‚úîÔ∏èü§ìüßêüëçüëåüôÜ‚Äç‚ôÇüôÜ‚Äç‚ôÄüôã‚Äç‚ôÇüôã‚Äç‚ôÄÔ∏è‚úåÔ∏èü§ôüëèüôåü§ùüôàüôâüôä‚ò∫Ô∏èüòòüòóüòôüòöüòΩüíãüëÑ‚ù§Ô∏è‚ù§Ô∏èüíòüíùüíñüíóüíìüíûüíïüíå‚ù£Ô∏èüß°üíõüíöüíôüíúüñ§‚ô•üíüüòçü•∞üòªüë®‚Äç‚ù§Ô∏è‚Äçüíã‚Äçüë®üë©‚Äç‚ù§Ô∏è‚Äçüíã‚Äçüë©üë®‚Äç‚ù§Ô∏è‚Äçüë®üë©‚Äç‚ù§Ô∏è‚Äçüë©üíëüíçüë∞üå∏üíÆüèµüåπü•Äüå∫üåªüåºüå∑üíêüòîüòìüòûüòü‚òπÔ∏èüôÅüò¢üò≠üò¶üòßüòøüò•üò´üò∞üò®ü•∫üò©üíîü§¶‚Äç‚ôÇÔ∏èü§¶‚Äç‚ôÄÔ∏èüò™üò¥üò∑ü§íü§ïü§¢ü§Æü§ßüò∂ü§êüòèüòºüòàüòíü§®üòêüòëüò¨üôÇüôÉüòïü§îüôÑüòñüòµü•¥ü•µüò±üò≥üòÆüòØüôÄüò≤üò°üò†ü§¨üò§ü§Øüòæüëøüò£ü§•ü§´üí©üñïüëäüëéüôÖ‚Äç‚ôÇÔ∏èüôÖ‚Äç‚ôÄÔ∏è‚úå])\1{3,}',
                r'\1\1\1', self.cleanedData[i])
            self.cleanedData[i] = re.sub('\ÿü+', ' ÿü ', self.cleanedData[i])
            self.cleanedData[i] = re.sub('\!+', ' ! ', self.cleanedData[i])
            self.cleanedData[i] = re.sub('\.+', ' . ', self.cleanedData[i])
            return

    def final_pre_processing_step(self):
        for i in range(len(self.cleanedData)):
            self.cleanedData[i] = re.sub(
                '\/+|\‚óè+|\‚óΩ+|\Ÿ™+|\‚ñ™+|\¬ª+|\¬´+|\_+|\ ö√Ø…û+|\"+|\-+|\:+|\@+|\#+|\$+|\Ô∑∫+|\%+|\^+|\&+|\(+|\)+|\.+|\,+|\?+|\=+|\++|\ÿõ+\‚Äú+|\‚Äù+',
                ' ', self.cleanedData[i])
            self.cleanedData[i] = re.sub('\!', ' ! ', self.cleanedData[i])
            self.cleanedData[i] = re.sub('\ÿü ', 'ÿü ', self.cleanedData[i])
            self.cleanedData[i] = re.sub('\.', ' . ', self.cleanedData[i])
            self.cleanedData[i] = re.sub('\s+', ' ', self.cleanedData[i])
            self.cleanedData[i] = re.sub('\_+', ' ', self.cleanedData[i])
            self.cleanedData[i] = re.sub('\‚Ä¶+', ' ', self.cleanedData[i])
            self.cleanedData[i] = re.sub('\‚Äú|\‚Äù', '', self.cleanedData[i])

            return

    def full_processing(self, ex_words=[]):
        
        self.initial_pre_processing()
        self.remove_hashtags(1)
        self.remove_stop_words()
        self.normalize()
        self.final_pre_processing_step()
        self.de_emojize()
        self.de_noise()
        return
        
        
# if __name__ == "__main__":
    # import pandas as pd
    # data['content'] = pd.read_csv('SentimentDSPreprocessed.csv',names=['content'])
    # p = GeneralProcessingService(data['content']) 
    # return p.cleanedData
    
