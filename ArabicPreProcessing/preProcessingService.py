import regex as re
import nltk
import os
from ArabicPreProcessing.helper import Helper

class GeneralProcessingService:
    emojies_uni = {

        u': ğŸ˜‚ :': u'ğŸ˜‚',
        u': ğŸ¤£ :': u'ğŸ¤£',
        u': ğŸ˜† :': u'ğŸ˜†',
        u': ğŸ˜… :': u'ğŸ˜…',
        u': ğŸ˜¹ :': u'ğŸ˜¹',
        u': ğŸ˜› :': u'ğŸ˜›',
        u': ğŸ˜œ :': u'ğŸ˜œ',
        u': ğŸ˜ :': u'ğŸ˜',
        u': ğŸ¸ :': u'ğŸ¸',
        u': ğŸ :': u'ğŸ',

        u': ğŸ˜€ :': u'ğŸ˜€',
        u': ğŸ˜ :': u'ğŸ˜',
        u': ğŸ˜ƒ :': u'ğŸ˜ƒ',
        u': ğŸ˜„ :': u'ğŸ˜„',
        u': ğŸ˜¸ :': u'ğŸ˜¸',
        u': ğŸ˜º :': u'ğŸ˜º',
        u': ğŸ¤­ :': u'ğŸ¤­',
        u': ğŸ˜‡ :': u'ğŸ˜‡',
        u': ğŸ¤  :': u'ğŸ¤ ',
        u': ğŸ˜Š :': u'ğŸ˜Š',
        u': ğŸ¤— :': u'ğŸ¤—',
        u': ğŸ˜ :': u'ğŸ˜',
        u': ğŸŒš :': u'ğŸŒš',
        u': ğŸŒ :': u'ğŸŒ',
        u': ğŸ˜‰ :': u'ğŸ˜‰',
        u': ğŸ˜‹ :': u'ğŸ˜‹',
        u': ğŸ¤¤ :': u'ğŸ¤¤',
        u': ğŸ¤© :': u'ğŸ¤©',
        u': ğŸ¥³ :': u'ğŸ¥³',
        u': ğŸ”¥ :': u'ğŸ”¥',
        u': ğŸ˜Œ :': u'ğŸ˜Œ',
        u': ğŸ¤‘ :': u'ğŸ¤‘',
        u': ğŸ’ƒ :': u'ğŸ’ƒ',
        u': ğŸ’ª :': u'ğŸ’ª',
        u': âœ” :': u'âœ”',
        u': âœ”ï¸ :': u'âœ”ï¸',
        u': ğŸ¤“ :': u'ğŸ¤“',
        u': ğŸ§ :': u'ğŸ§',
        u': ğŸ‘ :': u'ğŸ‘',
        u': ğŸ‘Œ :': u'ğŸ‘Œ',
        u': ğŸ™†â€â™‚ï¸ :': u'ğŸ™†â€â™‚ï¸',
        u': ğŸ™†â€â™€ï¸ :': u'ğŸ™†â€â™€ï¸',
        u': ğŸ™‹â€â™‚ï¸ :': u'ğŸ™‹â€â™‚ï¸',
        u': ğŸ™‹â€â™€ï¸ :': u'ğŸ™‹â€â™€ï¸',
        u': âœŒï¸ :': u'âœŒï¸',
        u': ğŸ¤™ :': u'ğŸ¤™',
        u': ğŸ‘ :': u'ğŸ‘',
        u': ğŸ™Œ :': u'ğŸ™Œ',
        u': ğŸ¤ :': u'ğŸ¤',
        u': ğŸ™ˆ :': u'ğŸ™ˆ',
        u': ğŸ™‰ :': u'ğŸ™‰',
        u': ğŸ™Š :': u'ğŸ™Š',
        u': â˜ºï¸ :': u'â˜ºï¸',

        u': ğŸ˜˜ :': u'ğŸ˜˜',
        u': ğŸ˜— :': u'ğŸ˜—',
        u': ğŸ˜™ :': u'ğŸ˜™',
        u': ğŸ˜š :': u'ğŸ˜š',
        u': ğŸ˜½ :': u'ğŸ˜½',
        u': ğŸ’‹ :': u'ğŸ’‹',
        u': ğŸ‘„ :': u'ğŸ‘„',
        u': â¤ï¸ :': u'â¤ï¸',
        u': â¤ï¸ :': u'â¤ï¸',
        u': ğŸ’˜ :': u'ğŸ’˜',
        u': ğŸ’ :': u'ğŸ’',
        u': ğŸ’– :': u'ğŸ’–',
        u': ğŸ’— :': u'ğŸ’—',
        u': ğŸ’“ :': u'ğŸ’“',
        u': ğŸ’ :': u'ğŸ’',
        u': ğŸ’• :': u'ğŸ’•',
        u': ğŸ’Œ :': u'ğŸ’Œ',
        u': â£ï¸ :': u'â£ï¸',
        u': ğŸ§¡ :': u'ğŸ§¡',
        u': ğŸ’› :': u'ğŸ’›',
        u': ğŸ’š :': u'ğŸ’š',
        u': ğŸ’™ :': u'ğŸ’™',
        u': ğŸ’œ :': u'ğŸ’œ',
        u': ğŸ–¤ :': u'ğŸ–¤',
        u': â™¥ :': u'â™¥',
        u': ğŸ’Ÿ :': u'ğŸ’Ÿ',
        u': ğŸ˜ :': u'ğŸ˜',
        u': ğŸ¥° :': u'ğŸ¥°',
        u': ğŸ˜» :': u'ğŸ˜»',
        u': ğŸ‘¨â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ :': u'ğŸ‘¨â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨',
        u': ğŸ‘©â€â¤ï¸â€ğŸ’‹â€ğŸ‘© :': u'ğŸ‘©â€â¤ï¸â€ğŸ’‹â€ğŸ‘©',
        u': ğŸ‘¨â€â¤ï¸â€ğŸ‘¨ :': u'ğŸ‘¨â€â¤ï¸â€ğŸ‘¨',
        u': ğŸ‘©â€â¤ï¸â€ğŸ‘© :': u'ğŸ‘©â€â¤ï¸â€ğŸ‘©',
        u': ğŸ’‘ :': u'ğŸ’‘',
        u': ğŸ’ :': u'ğŸ’',
        u': ğŸ‘° :': u'ğŸ‘°',
        u': ğŸŒ¸ :': u'ğŸŒ¸',
        u': ğŸ’® :': u'ğŸ’®',
        u': ğŸµ :': u'ğŸµ',
        u': ğŸŒ¹ :': u'ğŸŒ¹',
        u': ğŸ¥€ :': u'ğŸ¥€',
        u': ğŸŒº :': u'ğŸŒº',
        u': ğŸŒ» :': u'ğŸŒ»',
        u': ğŸŒ¼ :': u'ğŸŒ¼',
        u': ğŸŒ· :': u'ğŸŒ·',
        u': ğŸ’ :': u'ğŸ’',
        u': âœŒ :': u'âœŒ',
        u': ğŸ˜” :': u'ğŸ˜”',
        u': ğŸ˜“ :': u'ğŸ˜“',
        u': ğŸ˜ :': u'ğŸ˜',
        u': ğŸ˜Ÿ :': u'ğŸ˜Ÿ',
        u': â˜¹ï¸ :': u'â˜¹ï¸',
        u': ğŸ™ :': u'ğŸ™',
        u': ğŸ˜¢ :': u'ğŸ˜¢',
        u': ğŸ˜­ :': u'ğŸ˜­',
        u': ğŸ˜¦ :': u'ğŸ˜¦',
        u': ğŸ˜§ :': u'ğŸ˜§',
        u': ğŸ˜¿ :': u'ğŸ˜¿',
        u': ğŸ˜¥ :': u'ğŸ˜¥',
        u': ğŸ˜« :': u'ğŸ˜«',
        u': ğŸ˜° :': u'ğŸ˜°',
        u': ğŸ˜¨ :': u'ğŸ˜¨',
        u': ğŸ¥º :': u'ğŸ¥º',
        u': ğŸ˜© :': u'ğŸ˜©',
        u': ğŸ’” :': u'ğŸ’”',
        u': ğŸ¤¦â€â™‚ï¸ :': u'ğŸ¤¦â€â™‚ï¸',
        u': ğŸ¤¦â€â™€ï¸ :': u'ğŸ¤¦â€â™€ï¸',
        u': ğŸ˜ª :': u'ğŸ˜ª',
        u': ğŸ˜´ :': u'ğŸ˜´',
        u': ğŸ˜· :': u'ğŸ˜·',
        u': ğŸ¤’ :': u'ğŸ¤’',
        u': ğŸ¤• :': u'ğŸ¤•',
        u': ğŸ¤¢ :': u'ğŸ¤¢',
        u': ğŸ¤® :': u'ğŸ¤®',
        u': ğŸ¤§ :': u'ğŸ¤§',
        u': ğŸ˜¶ :': u'ğŸ˜¶',
        u': ğŸ¤ :': u'ğŸ¤',
        u': ğŸ˜ :': u'ğŸ˜',
        u': ğŸ˜¼ :': u'ğŸ˜¼',
        u': ğŸ˜ˆ :': u'ğŸ˜ˆ',
        u': ğŸ˜’ :': u'ğŸ˜’',
        u': ğŸ¤¨ :': u'ğŸ¤¨',
        u': ğŸ˜ :': u'ğŸ˜',
        u': ğŸ˜‘ :': u'ğŸ˜‘',
        u': ğŸ˜¬ :': u'ğŸ˜¬',
        u': ğŸ™‚ :': u'ğŸ™‚',
        u': ğŸ™ƒ :': u'ğŸ™ƒ',
        u': ğŸ˜• :': u'ğŸ˜•',
        u': ğŸ¤” :': u'ğŸ¤”',
        u': ğŸ™„ :': u'ğŸ™„',
        u': ğŸ˜– :': u'ğŸ˜–',
        u': ğŸ˜µ :': u'ğŸ˜µ',
        u': ğŸ¥´ :': u'ğŸ¥´',
        u': ğŸ¥µ :': u'ğŸ¥µ',
        u': ğŸ˜± :': u'ğŸ˜±',
        u': ğŸ˜³ :': u'ğŸ˜³',
        u': ğŸ˜® :': u'ğŸ˜®',
        u': ğŸ˜¯ :': u'ğŸ˜¯',
        u': ğŸ™€ :': u'ğŸ™€',
        u': ğŸ˜² :': u'ğŸ˜²',
        u': ğŸ˜¡ :': u'ğŸ˜¡',
        u': ğŸ˜  :': u'ğŸ˜ ',
        u': ğŸ¤¬ :': u'ğŸ¤¬',
        u': ğŸ˜¤ :': u'ğŸ˜¤',
        u': ğŸ¤¯ :': u'ğŸ¤¯',
        u': ğŸ˜¾ :': u'ğŸ˜¾',
        u': ğŸ‘¿ :': u'ğŸ‘¿',
        u': ğŸ˜£ :': u'ğŸ˜£',
        u': ğŸ¤¥ :': u'ğŸ¤¥',
        u': ğŸ¤« :': u'ğŸ¤«',
        u': ğŸ’© :': u'ğŸ’©',
        u': ğŸ–• :': u'ğŸ–•',
        u': ğŸ‘Š :': u'ğŸ‘Š',
        u': ğŸ‘ :': u'ğŸ‘',
        u': ğŸ™…â€â™‚ï¸ :': u'ğŸ™…â€â™‚ï¸',
        u': ğŸ™†â€â™€ :': u'ğŸ™†â€â™€',
        u': ğŸ™…â€â™€ï¸ :': u'ğŸ™…â€â™€ï¸',  ######
        u': ğŸ’†â€â™‚ :': u'ğŸ’†â€â™‚',
        u': ğŸ’ :': u'ğŸ’',
        u': ğŸ¤ª :': u'ğŸ¤ª',
        u': â¤ :': u'â¤',
        u': â˜» :': u'â˜»',
        u': ğŸ™‹â€â™€ :': u'ğŸ™‹â€â™€',
        u': â˜» :': u'â˜»',
        u': ğŸ’£ :': u'ğŸ’£',
        u': ğŸƒ :': u'ğŸƒ',
        u': ğŸ¤·â€â™‚ :': u'ğŸ¤·â€â™‚',
        u': ğŸ¤¦â€â™€ :': u'ğŸ¤¦â€â™€',
        u': âŒ :': u'âŒ',
        u': â˜¹ :': u'â˜¹',
        u': ğŸ” :': u'ğŸ”',
        u': ğŸ“¿ :': u'ğŸ“¿',
        u': ğŸµ :': u'ğŸµ',
        u': ğŸ¶ :': u'ğŸ¶',
        u': ğŸ¼ :': u'ğŸ¼',
        u': âœŠ :': u'âœŠ',
        u': ğŸ¤¦â€â™‚ :': u'ğŸ¤¦â€â™‚',
        u': ğŸ’†ğŸ»â€â™€ :': u'ğŸ’†ğŸ»â€â™€',

    }
    emojies_dict = {v: k for k, v in emojies_uni.items()}

    def __init__(self, input_data=[],data_path=''):
        '''input data as list through input_data, or as file through data_path,
        output pre-processed data as list'''
        import pandas as pd

        self.data = pd.DataFrame()
        if len(input_data) != 0:
            self.data['content'] = input_data

        settings_dir = os.path.dirname('__file__')
        PROJECT_ROOT = os.path.abspath(os.path.dirname(settings_dir))
        stop_word_file_path = PROJECT_ROOT + '\\resources\\stopwords.csv'

        stop_wrods = pd.read_csv(stop_word_file_path, error_bad_lines=False, names=['stopword'])
        self.stop_words = list(stop_wrods['stopword'])
        
        if data_path != '':
            self.data['content'] = pd.read_csv(data_path, error_bad_lines=False, names=['content'])

        self.cleanedData = []
        self.hashTags = []
        self.excludedWords = []
        self.new_tokenized_data  = []

        tokenized_data = [nltk.word_tokenize(item) for item in self.data.content]
        for i in range(len(tokenized_data)):
            temp = []
            for j in range(len(tokenized_data[i])):
                if len(tokenized_data[i][j]) > 2 and  not(self.sum_func(tokenized_data[i][j]) ):
                    temp.append(tokenized_data[i][j])
            self.new_tokenized_data.append(temp)

    def sum_func(self,text):
        match = re.search(r'(([.?#@+])\2{2,})', text)
        return match

    def de_emojize(self):
        self.cleanedData = Helper(GeneralProcessingService.emojies_dict).deNoise(self.cleanedData)
        return

    def remove_names(self, lst):
        flatten =[item for sublist in lst for item in sublist]
        self.cleanedData = Helper(flatten).deNoise(self.cleanedData)
        return

    def initial_pre_processing(self):
        temp = list((self.data['content']))
        self.cleanedData += [re.sub(r'[A-Za-z]+|[\u0400-\u0500]+', '', str(item)) for item in temp]
        return

    def remove_hashtags(self, first_time=1):
        if first_time == 1:
            for i in range(len(self.cleanedData)):
                self.cleanedData[i] = str(' '.join(
                    re.sub("([0-9]+)|([A-Za-z]+)|\_+|(\#)+|(\/)+|(\:)+", " ", self.cleanedData[i]).split()))
            return

    def remove_mentions(self, mentions=[]):
        WORD = re.compile(r'\w+')

        flatten = []
        for i in range(len(mentions)):
            temp = [c for c in WORD.findall(mentions[i])]
            if len(temp) > 1:
                for j in temp:
                    flatten.append(j)
            elif len(temp) == 1:
                flatten.append(temp[0])
        for i in range(len(self.cleanedData)):
            # self.hashTags.append(re.findall(r"#(\w+)", str(i)))
            # self.data['content'][i] = str(' '.join(
            #     c for c in WORD.findall(self.data['content'][i]) if c not in flatten and c != ''))
            self.cleanedData[i] = str(' '.join(
                c for c in WORD.findall(self.cleanedData[i]) if c not in flatten and c != ''))
        return

    def de_noise(self):
        from string import punctuation
        noise = re.compile(""" Ù‘    | # Tashdid
                                 Ù    | # Fatha
                                 Ù‹    | # Tanwin Fath
                                 Ù    | # Damma
                                 ÙŒ    | # Tanwin Damm
                                 Ù    | # Kasra
                                 Ù    | # Tanwin Kasr
                                 Ù’    | # Sukun
                                 Ù€   |  # Tatwil/Kashida
                                 ØŒ   |
                             """, re.VERBOSE)
        flagsUs = re.compile("["
                             u"\U0001F680-\U0001F6FF"  # transport & map symbols
                             u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                             "]+", flags=re.UNICODE)
        dirtyChars = re.compile("["
                                "\u0600-\u0620"
                                "\u063B-\u0640"
                                "\u064B-\u065F"
                                "\u066A-\u06FF"
                                "\u0750-\u077F"
                                "\u08A0-\u08FF"
                                "\uFB50-\uFBE9"
                                "\uFBF0-\uFBFB"
                                "\uFC5B-\uFC63"
                                "\uFCF2-\uFCF4"
                                "\uFD3C-\uFD4F"
                                "\uFD90-\uFD91"
                                "\uFDC8-\uFDFF"
                                "\uFE70-\uFE7F"
                                "\uFEFD-\uFEFF"
                                "]+", flags=re.UNICODE)
        for i in range(len(self.cleanedData)):
            self.cleanedData[i] = str(re.sub(flagsUs, '', self.cleanedData[i]))
            self.cleanedData[i] = str(re.sub(dirtyChars, '', self.cleanedData[i]))
            self.cleanedData[i] = str(re.sub(noise, '', self.cleanedData[i]))
            self.cleanedData[i] = str(''.join(c for c in self.cleanedData[i] if c not in punctuation))
        return

    def remove_stop_words(self):
        for i in range(len(self.cleanedData)):
            self.cleanedData[i] = (
                str(' '.join(
                    c for c in nltk.word_tokenize(self.cleanedData[i]) if c not in self.stop_words and c != '')))
        return

    def normalize(self):
        for i in range(len(self.cleanedData)):
            self.cleanedData[i] = re.sub("[Ø¥Ø£Ù±Ø¢Ø§]", "Ø§", self.cleanedData[i])
            self.cleanedData[i] = re.sub("Ù‰", "ÙŠ", self.cleanedData[i])
            self.cleanedData[i] = re.sub("Ø¤", "Ø¡", self.cleanedData[i])
            self.cleanedData[i] = re.sub("[ï»·]", "Ù„Ø§", self.cleanedData[i])
            self.cleanedData[i] = re.sub("Ø©", "Ù‡", self.cleanedData[i])
            self.cleanedData[i] = re.sub("Ø¦", "Ø¡", self.cleanedData[i])
            self.cleanedData[i] = ''.join([i for i in self.cleanedData[i] if not i.isdigit()])
            self.cleanedData[i] = re.sub(r'([\u0600-\u06FF])\1{3,}', r'\1\1\1', self.cleanedData[i])
            self.cleanedData[i] = re.sub(
                r'([ğŸ˜‚ğŸ¤ªğŸ¤£ğŸ˜†ğŸ˜…ğŸ˜¹ğŸ˜›ğŸ˜œğŸ˜ğŸ¸ğŸğŸ˜€ğŸ˜ğŸ˜ƒğŸ˜„ğŸ˜¸ğŸ˜ºğŸ¤­ğŸ˜‡ğŸ¤ ğŸ˜ŠğŸ¤—ğŸ˜ğŸŒšğŸŒğŸ˜‰ğŸ˜‹ğŸ¤¤ğŸ¤©ğŸ¥³ğŸ”¥ğŸ˜ŒğŸ¤‘ğŸ’ƒğŸ’ªâœ”âœ”ï¸ğŸ¤“ğŸ§ğŸ‘ğŸ‘ŒğŸ™†â€â™‚ğŸ™†â€â™€ğŸ™‹â€â™‚ğŸ™‹â€â™€ï¸âœŒï¸ğŸ¤™ğŸ‘ğŸ™ŒğŸ¤ğŸ™ˆğŸ™‰ğŸ™Šâ˜ºï¸ğŸ˜˜ğŸ˜—ğŸ˜™ğŸ˜šğŸ˜½ğŸ’‹ğŸ‘„â¤ï¸â¤ï¸ğŸ’˜ğŸ’ğŸ’–ğŸ’—ğŸ’“ğŸ’ğŸ’•ğŸ’Œâ£ï¸ğŸ§¡ğŸ’›ğŸ’šğŸ’™ğŸ’œğŸ–¤â™¥ğŸ’ŸğŸ˜ğŸ¥°ğŸ˜»ğŸ‘¨â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ‘©â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ‘¨â€â¤ï¸â€ğŸ‘¨ğŸ‘©â€â¤ï¸â€ğŸ‘©ğŸ’‘ğŸ’ğŸ‘°ğŸŒ¸ğŸ’®ğŸµğŸŒ¹ğŸ¥€ğŸŒºğŸŒ»ğŸŒ¼ğŸŒ·ğŸ’ğŸ˜”ğŸ˜“ğŸ˜ğŸ˜Ÿâ˜¹ï¸ğŸ™ğŸ˜¢ğŸ˜­ğŸ˜¦ğŸ˜§ğŸ˜¿ğŸ˜¥ğŸ˜«ğŸ˜°ğŸ˜¨ğŸ¥ºğŸ˜©ğŸ’”ğŸ¤¦â€â™‚ï¸ğŸ¤¦â€â™€ï¸ğŸ˜ªğŸ˜´ğŸ˜·ğŸ¤’ğŸ¤•ğŸ¤¢ğŸ¤®ğŸ¤§ğŸ˜¶ğŸ¤ğŸ˜ğŸ˜¼ğŸ˜ˆğŸ˜’ğŸ¤¨ğŸ˜ğŸ˜‘ğŸ˜¬ğŸ™‚ğŸ™ƒğŸ˜•ğŸ¤”ğŸ™„ğŸ˜–ğŸ˜µğŸ¥´ğŸ¥µğŸ˜±ğŸ˜³ğŸ˜®ğŸ˜¯ğŸ™€ğŸ˜²ğŸ˜¡ğŸ˜ ğŸ¤¬ğŸ˜¤ğŸ¤¯ğŸ˜¾ğŸ‘¿ğŸ˜£ğŸ¤¥ğŸ¤«ğŸ’©ğŸ–•ğŸ‘ŠğŸ‘ğŸ™…â€â™‚ï¸ğŸ™…â€â™€ï¸âœŒ])\1{3,}',
                r'\1\1\1', self.cleanedData[i])
            self.cleanedData[i] = re.sub('\ØŸ+', ' ØŸ ', self.cleanedData[i])
            self.cleanedData[i] = re.sub('\!+', ' ! ', self.cleanedData[i])
            self.cleanedData[i] = re.sub('\.+', ' . ', self.cleanedData[i])
            return

    def final_pre_processing_step(self):
        for i in range(len(self.cleanedData)):
            self.cleanedData[i] = re.sub(
                '\/+|\â—+|\â—½+|\Ùª+|\â–ª+|\Â»+|\Â«+|\_+|\ÊšÃ¯É+|\"+|\-+|\:+|\@+|\#+|\$+|\ï·º+|\%+|\^+|\&+|\(+|\)+|\.+|\,+|\?+|\=+|\++|\Ø›+\â€œ+|\â€+',
                ' ', self.cleanedData[i])
            self.cleanedData[i] = re.sub('\!', ' ! ', self.cleanedData[i])
            self.cleanedData[i] = re.sub('\ØŸ ', 'ØŸ ', self.cleanedData[i])
            self.cleanedData[i] = re.sub('\.', ' . ', self.cleanedData[i])
            self.cleanedData[i] = re.sub('\s+', ' ', self.cleanedData[i])
            self.cleanedData[i] = re.sub('\_+', ' ', self.cleanedData[i])
            self.cleanedData[i] = re.sub('\â€¦+', ' ', self.cleanedData[i])
            self.cleanedData[i] = re.sub('\â€œ|\â€', '', self.cleanedData[i])

            return

    def full_processing(self, ex_words=[]):
        
        self.initial_pre_processing()
        self.remove_hashtags(1)
        self.remove_stop_words()
        self.normalize()
        self.final_pre_processing_step()
        self.de_emojize()
        self.de_noise()
        return
        
        
# if __name__ == "__main__":
    # import pandas as pd
    # data['content'] = pd.read_csv('SentimentDSPreprocessed.csv',names=['content'])
    # p = GeneralProcessingService(data['content']) 
    # return p.cleanedData
    
